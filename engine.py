import torch
from diffusers import StableDiffusionPipeline
from PIL import Image
import numpy as np
import gc
import os
import base64
from io import BytesIO
import uuid

# Define constants
CACHED_MODEL_PATH = r'C:\Users\mabhi\Documents\comic\cached_model'
ORIGINAL_MODEL_ID = "CompVis/stable-diffusion-v1-4"
OUTPUT_FOLDER = r'C:\Users\mabhi\Documents\comic\output\pics'

def load_model(use_original=False):
    """Loads the Stable Diffusion model."""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    try:
        if use_original:
            print(f"Loading original model: {ORIGINAL_MODEL_ID}")
            pipeline = StableDiffusionPipeline.from_pretrained(ORIGINAL_MODEL_ID, torch_dtype=torch.float16, safety_checker=None)
        else:
            print(f"Loading fine-tuned model from: {CACHED_MODEL_PATH}")
            pipeline = StableDiffusionPipeline.from_pretrained(CACHED_MODEL_PATH, torch_dtype=torch.float16, safety_checker=None)
        
        pipeline = pipeline.to(device)
        pipeline.enable_attention_slicing(1)
        pipeline.enable_vae_slicing()
        
        print("Model loaded successfully")
        print(f"UNet parameters: {sum(p.numel() for p in pipeline.unet.parameters())}")
        print(f"VAE parameters: {sum(p.numel() for p in pipeline.vae.parameters())}")
        print(f"Text Encoder parameters: {sum(p.numel() for p in pipeline.text_encoder.parameters())}")
        
        return pipeline
    except Exception as e:
        print(f"Error loading model: {str(e)}")
        return None

def generate_image(prompt, pipeline, num_inference_steps=50, guidance_scale=7.5, width=512, height=512, negative_prompt=None):
    """Generates an image from a prompt using the Stable Diffusion model."""
    print(f"Generating image with prompt: {prompt}")
    print(f"Negative prompt: {negative_prompt}")
    print(f"Inference steps: {num_inference_steps}, Guidance scale: {guidance_scale}")
    print(f"Image size: {width}x{height}")
    
    try:
        # Check CUDA memory
        if torch.cuda.is_available():
            print(f"CUDA memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
            print(f"CUDA memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB")
        
        with torch.no_grad():
            with torch.autocast("cuda"):
                output = pipeline(
                    prompt, 
                    negative_prompt=negative_prompt,
                    num_inference_steps=num_inference_steps, 
                    guidance_scale=guidance_scale,
                    width=width,
                    height=height,
                )
        
        if output.images is None or len(output.images) == 0:
            print("Error: No images generated by the pipeline")
            return None
        
        image = output.images[0]
        
        # Debug: Print image statistics
        img_array = np.array(image)
        print(f"Processed image shape: {img_array.shape}")
        print(f"Processed image min value: {img_array.min()}")
        print(f"Processed image max value: {img_array.max()}")
        print(f"Processed image mean value: {img_array.mean()}")
        
        if img_array.max() == 0:
            print("Error: Generated image is blank")
            return None
        
        # Convert PIL Image to base64 string
        buffered = BytesIO()
        image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()
        
        return img_str
    except Exception as e:
        print(f"Error generating image: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def save_image(image, filename):
    """Saves the generated image to the output folder."""
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    full_path = os.path.join(OUTPUT_FOLDER, filename)
    image.save(full_path)
    print(f"Image saved as {full_path}")

def main():
    """Main loop for generating images using the Stable Diffusion model."""
    use_original = input("Use original model? (y/n): ").lower() == 'y'
    pipeline = load_model(use_original)
    if pipeline is None:
        print("Failed to load model. Exiting.")
        return

    while True:
        prompt = input("Enter a prompt (or 'quit' to exit): ")
        if prompt.lower() == 'quit':
            break
        
        negative_prompt = input("Enter a negative prompt (optional): ")
        
        try:
            num_steps = int(input("Enter the number of inference steps (default 50, range 20-100): ") or 50)
            num_steps = max(20, min(100, num_steps))
            
            guidance_scale = float(input("Enter the guidance scale (default 7.5, recommended range 7-9): ") or 7.5)
            
            width = int(input("Enter image width (multiple of 8, max 1024, default 512): ") or 512)
            height = int(input("Enter image height (multiple of 8, max 1024, default 512): ") or 512)
            
            width = max(64, min(1024, width - width % 8))
            height = max(64, min(1024, height - height % 8))
            
            print(f"Generating image with {num_steps} inference steps at {width}x{height} resolution...")
            
            image_base64 = generate_image(prompt, pipeline, num_inference_steps=num_steps, guidance_scale=guidance_scale, 
                                   width=width, height=height, negative_prompt=negative_prompt)
            if image_base64:
                filename = input("Enter a filename to save the image (without extension): ") or f"generated_{prompt[:20].replace(' ', '_')}"
                filename = f"{filename}.png"
                
                # Convert base64 string back to image
                image = Image.open(BytesIO(base64.b64decode(image_base64)))
                save_image(image, filename)
            else:
                print("Failed to generate image.")
        except ValueError as e:
            print(f"Invalid input: {e}")
        except Exception as e:
            print(f"An unexpected error occurred: {e}")
        
        # Clear CUDA cache
        torch.cuda.empty_cache()
        gc.collect()
        print("CUDA cache cleared.")

if __name__ == "__main__":
    main()
